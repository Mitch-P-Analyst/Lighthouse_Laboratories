{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95d5a65",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) and Topic Modeling\n",
    "\n",
    "- LDA (Latent Dirichlet Allocation)\n",
    "- NMF (Non-Negative Matrix Factorization) for topic modeling\n",
    "\n",
    "\n",
    "- Learning Outcomes\n",
    "    - Generate bite-size blocks of code so that you can see how different steps fit together\n",
    "    - Deepen your understanding of NLP and LDA/topic modelling\n",
    "    - Practice using AI for redundant tasks\n",
    "\n",
    "\n",
    "- Use ChatGPT to answer the following questions before (or while) you perform the next tasks.\n",
    "    - What is the tfidfvectorizer?\n",
    "    - How is tfidf calculated?\n",
    "    - How does the countvectorizer differ from the tfidfvectorizer?\n",
    "    - How can I instantiate a TfidfVectorizer with the following parameters:\n",
    "        - max_df = 0.95\n",
    "        - min_df = 2\n",
    "        - max_features = no_features\n",
    "        - stop_words = 'english'\n",
    "    - What are the methods of the tfidfvectorizer object?\n",
    "    - What is NMF?\n",
    "    - What is LDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30fc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import TfidfVectorizer and CountVectorizer from sklearn\n",
    "from Sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# import fetch_20newsgroups from sklearn.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# import NMF and LatentDirichletAllocation from sklearn\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300377ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (468294634.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Create a variable called no_features and set its value to 100.\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Create a variable called no_features and set its value to 100.\n",
    "\n",
    "Then, create a variable no_topics and set its value to 100.\n",
    "\n",
    "NMF\n",
    "Instruction\n",
    "Instantiate a TfidfVectorizer with the following parameters:\n",
    "\n",
    "max_df = 0.95\n",
    "min_df = 2\n",
    "max_features = no_features\n",
    "stop_words = 'english'\n",
    "Instruction\n",
    "Then, use the fit_transform method of TfidfVectorizer to transform the documents.\n",
    "\n",
    "Instruction\n",
    "Next, get the features names from TfidfVectorizer.\n",
    "\n",
    "Instruction\n",
    "Finally, instantiate NMF and fit_transform data.\n",
    "\n",
    "LDA with Sklearn\n",
    "Instruction\n",
    "Instantiate a CountVectorizer with the following parameters:\n",
    "\n",
    "max_df = 0.95\n",
    "min_df = 2\n",
    "max_features = no_features\n",
    "stop_words = 'english'\n",
    "Instruction\n",
    "Then, use the fit_transform method of CountVectorizer to transform documents.\n",
    "\n",
    "Instruction\n",
    "Next, get the features names from CountVectorizer.\n",
    "\n",
    "Instruction\n",
    "Next, instantiate LatentDirichletAllocation and fit transformed data.\n",
    "\n",
    "Instruction\n",
    "Finally, create a function display_topics that is able to display the top words in a topic for different models.\n",
    "\n",
    "The expect outputs include:\n",
    "\n",
    "Display the top 10 words from each topic from NMF model\n",
    "Display the top 10 words from each topic from LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680131d5",
   "metadata": {},
   "source": [
    "Complete https://www.linkedin.com/pulse/nlp-a-complete-guide-topic-modeling-latent-dirichlet-sahil-m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5bb7cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33880021",
   "metadata": {},
   "outputs": [],
   "source": [
    "Install NLP Libraries\n",
    "Note\n",
    "You do not need to download all the libraries and packages mentioned in this exercise. Only selected libraries and packages will be relevant to you depending upon the project you choose to do for this course (choosing the project is the next task you do after this exercise). Feel free to discuss what packages are most relevant for your project with mentors or instructors.\n",
    "\n",
    "There are several libraries available for NLP tasks. For working with LLMs specifically, you might be interested in libraries such as transformers by Hugging Face, which provide interfaces to pre-trained models like GPT, BERT, and others. To install these, run:\n",
    "\n",
    "\n",
    "$ conda install transformers\n",
    "\n",
    "Install Additional Libraries (Optional)\n",
    "Depending on your NLP tasks, you may need additional libraries. Some common ones include:\n",
    "\n",
    "NLTK: A leading platform for building Python programs to work with human language data.\n",
    "\n",
    "spaCy: An industrial-strength NLP library for Python.\n",
    "\n",
    "Gensim: A robust semantic modeling library, useful for topic modeling and document similarity analysis.\n",
    "\n",
    "You can install these libraries using conda:\n",
    "\n",
    "\n",
    "\n",
    "$ conda install nltk spacy gensim\n",
    "\n",
    "\n",
    "Download Pre-Trained Models (Optional)\n",
    "Some tasks may require downloading pre-trained models or data. For instance, with transformers, you can download models directly in your code:\n",
    "\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "For NLTK, you might need to download certain data packages:\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "\n",
    "For spaCy, downloading a language model is often required:\n",
    "\n",
    "\n",
    "$ python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "Write Your NLP Code\n",
    "Now you are ready to write Python code for your NLP task. Here's a very simple example using the transformers library to generate text:\n",
    "\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Encode some input text\n",
    "input_ids = tokenizer.encode('As an example of NLP, ', return_tensors='pt')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Generate text until the output length (which includes the given input) reaches 50 tokens\n",
    "output = model.generate(input_ids, max_length=50, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode the output token ids\n",
    "decoded_output = tokenizer.decode(output[0])\n",
    "\n",
    "print(decoded_output)\n",
    "\n",
    "\n",
    "Remember that working with LLMs, especially when fine-tuning or training them, may require significant computational resources. For such cases, you might consider using cloud-based services with dedicated hardware like GPUs.\n",
    "\n",
    "Install PyTorch\n",
    "Instruction\n",
    "Install PyTorch with the help of the instructions given on PyTorch Get Started page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a62d415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
